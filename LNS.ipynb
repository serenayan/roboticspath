{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-QJjFZHIw74"
   },
   "source": [
    "# Decomposition for Solving Integer Programs\n",
    "## Introduction\n",
    "In this notebook, we will explore the idea of decomposition for solving integer programs (IPs). Integer programming is a general mathematical modeling tools for solving discrete optimization problems. It is widely used in industry, for example, [Amazon](https://www.truckstopsrouting.com/us/amazon-prime-same-day-deliveries/) relies on route optimization for planning deliveries, which can be modelled as IPs. [Sports scheduling](https://www.sciencedirect.com/science/article/abs/pii/S0305054812002869) is another area where IPs are widely used. The bad news is that solving general IPs are [NP-hard](https://en.wikipedia.org/wiki/Integer_programming). So solving an IP to optimality is often prohibitively expensive. In practice, general purpose IP solvers, such as Gurobi and SCIP, are used. These solvers implement the [branch-and-bound](https://en.wikipedia.org/wiki/Branch_and_bound) tree search algorithm together with a host of heuristics. Typically, these solvers are quite efficient at solving small scale problems with a small number of integer variables. However, once the number of integer variables becomes large, they become inefficient. A natural idea of mitigating this undesirable behavior is through the idea of decomposition. By breaking a large problem into a series of small problems, we can leverage existing strong solvers on the small problems. The main challenge is how to combine solutions generated from the small problems into a feasible solution to the original large problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LithlXmrct6f"
   },
   "source": [
    "## Setup\n",
    "We will use the [python-mip](https://www.python-mip.com/) package with an open-source IP solver [CBC](https://projects.coin-or.org/Cbc). We study the problem of computing a [maximum cut (MaxCut)](https://en.wikipedia.org/wiki/Maximum_cut) of a weighted graph. The MaxCut problem is defined over a weighted graph $G = (V, E)$. The problem aims to divide the vertex set $V$ into two disjoint subsets $V = V_0 \\cup V_1$ to maximize the total weights of cut edges, edges with one vertex in $V_0$ and the other in $V_1$. This problem can be formulated as IPs\n",
    ">$\\begin{align}\n",
    "& \\max \\sum_{(i, j)\\in E} w_{ij} e_{ij} \\\\\n",
    "& s.t.  \\\\ \n",
    "& e_{ij} \\le v_i + v_j, \\forall (i, j) \\in E \\\\\n",
    "& e_{ij} + v_i + v_j \\le 2, \\forall (i, j) \\in E \\\\\n",
    "& v_i \\in \\{0, 1\\}, \\forall i \\in V \\\\\n",
    "& e_{ij} \\in \\{0, 1\\}, \\forall (i, j) \\in E\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Intuitively, values for $v_i$ indicate in which subset it is placed. The variables $e_{ij}$ associated with each edge have the value 1 if it is a cut edge and the value 0 otherwise. \n",
    "\n",
    "The following code block generates IPs based on MaxCut problems. It has two components, the **gen_graph** function generates a random graph, with the option to specify which distribution to draw a graph from. Next, the **create_opt** function takes input a graph and outputs an IP based on the formuation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "O2JycHp0oFDe",
    "outputId": "81d18b43-fce2-4645-eb1f-08d08d16b874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9d/e8175387ecfe2827b1a08f0ca79cc6daa22299bef8f7062c06cf4ed45a3a/mip-1.8.0-py3-none-any.whl (47.6MB)\n",
      "\u001b[K     |████████████████████████████████| 47.6MB 85kB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from mip) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->mip) (2.20)\n",
      "Installing collected packages: mip\n",
      "Successfully installed mip-1.8.0\n",
      "Using Python-MIP package version 1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mip\n",
    "import mip\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def gen_graph(max_n, min_n, g_type='barabasi_albert', edge=4):\n",
    "    cur_n = np.random.randint(max_n - min_n + 1) + min_n\n",
    "    if g_type == 'erdos_renyi':\n",
    "        g = nx.erdos_renyi_graph(n = cur_n, p = 0.15)\n",
    "    elif g_type == 'powerlaw':\n",
    "        g = nx.powerlaw_cluster_graph(n = cur_n, m = 4, p = 0.05)\n",
    "    elif g_type == 'barabasi_albert':\n",
    "        g = nx.barabasi_albert_graph(n = cur_n, m = edge)\n",
    "    elif g_type == 'watts_strogatz':\n",
    "        g = nx.watts_strogatz_graph(n = cur_n, k = cur_n // 10, p = 0.1)\n",
    "\n",
    "    for edge in nx.edges(g):\n",
    "        g[edge[0]][edge[1]]['weight'] = random.uniform(0,1)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def getEdgeVar(m, v1, v2, vert):\n",
    "    u1 = min(v1, v2)\n",
    "    u2 = max(v1, v2)\n",
    "    if not ((u1, u2) in vert):\n",
    "        vert[(u1, u2)] = m.add_var(name='u%d_%d' %(u1, u2),\n",
    "                                   var_type='B')\n",
    "\n",
    "    return vert[(u1, u2)]\n",
    "\n",
    "\n",
    "def getNodeVar(m, v, node):\n",
    "    if not v in node:\n",
    "        node[v] = m.add_var(name='v%d' %v,\n",
    "                            var_type='B')\n",
    "\n",
    "    return node[v]\n",
    "\n",
    "\n",
    "def createOpt(G):\n",
    "    m = mip.Model()\n",
    "    # Emphasis is on finding good feasible solutions.\n",
    "    m.emphasis = 1\n",
    "    edgeVar = {}\n",
    "    nodeVar = {}\n",
    "    m.objective = 0\n",
    "  \n",
    "    for j, (v1, v2) in enumerate(G.edges()):\n",
    "        e12 = getEdgeVar(m, v1, v2, edgeVar)\n",
    "        node1 = getNodeVar(m, v1, nodeVar)\n",
    "        node2 = getNodeVar(m, v2, nodeVar)\n",
    "\n",
    "        m += e12 <= node1 + node2\n",
    "        m += e12 + node1 + node2 <= 2\n",
    "    \n",
    "        m.objective = m.objective - (G[v1][v2]['weight']) * e12\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def generateInstance(max_n, min_n, \n",
    "                     g_type='erdos_renyi', edge=4, outPrefix=None):\n",
    "    G = gen_graph(max_n, min_n, g_type, edge)\n",
    "    P = createOpt(G)\n",
    "\n",
    "    return G, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9zdTn1qin2o"
   },
   "outputs": [],
   "source": [
    "def generate_var_dict(model):\n",
    "    '''Returns a dictionary mapping nodes in a graph to variables in a model.'''\n",
    "    model_vars = model.vars\n",
    "    num_vars = 0\n",
    "    var_dict = {}\n",
    "\n",
    "    for model_var in model_vars:\n",
    "        if model_var.name.startswith('v'):\n",
    "            num_vars += 1\n",
    "\n",
    "    var_dict = dict([(i, [\"v%d\" %i]) for i in range(num_vars)])\n",
    "\n",
    "    return var_dict\n",
    "\n",
    "\n",
    "def uniform_random_clusters(var_dict, num_clusters):\n",
    "    '''Return a random clustering. Each node is assigned to a cluster\n",
    "    a equal probability.'''\n",
    "\n",
    "    choices = list(range(num_clusters))\n",
    "    clusters = dict([(i, []) for i in range(num_clusters)])\n",
    "\n",
    "    for k in var_dict.keys():\n",
    "        cluster_choice = random.choice(choices)\n",
    "        clusters[cluster_choice].append(k)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def initialize_solution(var_dict, model):\n",
    "    '''Initialize a feasible solution.\n",
    "\n",
    "    Arguments:\n",
    "      var_dict: a dict maps node index to node variable name.\n",
    "\n",
    "    Returns:\n",
    "      a dict maps node variable name to a value.\n",
    "      a torch tensor view of the dict.\n",
    "    '''\n",
    "\n",
    "    sol = {}\n",
    "    # the sol_vec needs to be of type float for later use with Pytorch.\n",
    "    sol_vec = None\n",
    "    init_obj = 0\n",
    "\n",
    "    for k, var_list in var_dict.items():\n",
    "        #sol_vec = np.zeros((len(var_dict), len(var_list)))\n",
    "        for i, v in enumerate(var_list):\n",
    "            sol[v] = 0      \n",
    "\n",
    "    return sol, init_obj    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yfzWH10Mi5F"
   },
   "source": [
    "## Large Neighborhood Search (LNS)\n",
    "We now describe the details of our LNS framework. At a high level, our LNS framework operates on an integer program (IP) via defining decompositions of its integer variables into disjoint subsets. Afterwards, we can select a subset and use an existing solver to optimize the variables in that subset while holding all other variables fixed. The benefit of this framework is that it is completely generic to any IP instantiation of any combinatorial optimization problem.\n",
    "\n",
    "For an integer program $P$ with a set of integer variables $X$ (not necessarily all the integer variables), we define a decomposition of the set $X$ as a disjoint union $X_1 \\cup X_2 \\cup \\cdots \\cup X_k$. Assume we have an existing feasible solution $S_X$ to $P$, we view each subset $X_i$ of integer variables as a local neighborhood for search. We fix integers in $X\\setminus X_i$ with their values in the current solution $S_X$ and optimize for variable in $X_i$ (referred as the $\\texttt{FIX_AND_OPTIMIZE}$ function in Line 3 of Alg 1). As the resulting optimization is a smaller IP, we can use any off-the-shelf IP solver to carry out the local search. In our experiments, we use Gurobi to optimize the sub-IP. A new solution is obtained and we repeat the process with the remaining subsets.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1MJTIMV186ltfohVbZNPArxL9lEueRD4J\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pax5vazc-gCC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def LNS(model, num_clusters, steps, time_limit=3, verbose=False):\n",
    "    \"\"\"Perform large neighborhood search (LNS) on model. The descent order is random.\n",
    "\n",
    "    Arguments:\n",
    "      model: the integer program.\n",
    "      num_clusters: number of clusters.\n",
    "      steps: the number of decompositions to apply.\n",
    "      var_dict: a dict maps node index to node variable name.\n",
    "      time_limit: time limit for each LNS step.\n",
    "      sol: (optional) initial solution.\n",
    "      graph: networkx graph object.\n",
    "      verbose: if True, print objective after every decomposition.\n",
    "    \"\"\"\n",
    "\n",
    "    model.max_seconds = time_limit\n",
    "    total_time = 0\n",
    "\n",
    "    var_dict = generate_var_dict(model)\n",
    "\n",
    "    start_time = time.time()\n",
    "    sol, start_obj = initialize_solution(var_dict, model)\n",
    "\n",
    "    cluster_list = []\n",
    "    obj_list = [start_obj]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        clusters = uniform_random_clusters(var_dict, num_clusters)\n",
    "\n",
    "        sol, solver_time, obj = LNS_by_clusters(\n",
    "            model.copy(), clusters, var_dict, sol)\n",
    "        total_time += solver_time\n",
    "        if verbose:\n",
    "            print(\"objective: \", obj)\n",
    "\n",
    "        cur_time = time.time()\n",
    "\n",
    "        cluster_list.append(clusters)\n",
    "        obj_list.append(obj)\n",
    "\n",
    "    return total_time, obj, start_obj - obj, cluster_list, obj_list\n",
    "\n",
    "\n",
    "def LNS_by_clusters(model, clusters, var_dict, sol):\n",
    "    \"\"\"Perform LNS by clusters. The order of clusters is from the largest to\n",
    "       the smallest.\n",
    "\n",
    "    Arguments:\n",
    "      model: the integer program.\n",
    "      clusters: a dict mapping cluster index to node index.\n",
    "      var_dict: mapping node index to node variable name.\n",
    "      sol: current solution.\n",
    "\n",
    "    Returns:\n",
    "      new solution. time spent by the solver. new objective.\n",
    "    \"\"\"\n",
    "\n",
    "    # order clusters by size from largest to smallest\n",
    "    sorted_clusters = sorted(clusters.items(), \n",
    "                             key=lambda x: len(x[1]), \n",
    "                             reverse=True)\n",
    "    solver_t = 0\n",
    "\n",
    "    for idx, cluster in sorted_clusters:\n",
    "        sol, solver_time, obj = gradient_descent(\n",
    "            model.copy(), cluster, var_dict, sol)\n",
    "        solver_t += solver_time\n",
    "\n",
    "    return sol, solver_t, obj\n",
    "\n",
    "\n",
    "def gradient_descent(model, cluster, var_dict, sol):\n",
    "    \"\"\"Perform gradient descent on model along coordinates defined by \n",
    "       variables in cluster,  starting from a current solution sol.\n",
    "    \n",
    "    Arguments:\n",
    "      model: the integer program.\n",
    "      cluster: the coordinates to perform gradient descent on.\n",
    "      var_dict: mapping node index to node variable name.\n",
    "      sol: a dict representing the current solution.\n",
    "\n",
    "    Returns:\n",
    "      new_sol: the new solution.\n",
    "      time: the time used to perform the descent.\n",
    "      obj: the new objective value.\n",
    "    \"\"\"\n",
    "\n",
    "    var_starts = []\n",
    "    for k, var_list in var_dict.items():\n",
    "        for v in var_list:\n",
    "            # warm start variables in the current coordinate set with the existing solution.\n",
    "            model_var = model.var_by_name(v)\n",
    "            if k in cluster:\n",
    "                var_starts.append((model_var, sol[v]))\n",
    "            else:\n",
    "                model += model_var == sol[v]\n",
    "\n",
    "    # model.start = var_starts\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.optimize()\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    new_sol = {}\n",
    "\n",
    "    for k, var_list in var_dict.items():\n",
    "        for v in var_list:\n",
    "            var = model.var_by_name(v)\n",
    "            try:\n",
    "                new_sol[v] = round(var.x)\n",
    "            except:\n",
    "                return sol, run_time, -1\n",
    "\n",
    "    return new_sol, run_time, model.objective_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbnNkJQTR2QC"
   },
   "source": [
    "Now let's try this idea out with random decompositions. We first generate a random graph sampled according to the [Barabasi-Albert model](https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8fpTV9FuTk8"
   },
   "outputs": [],
   "source": [
    "G, P = generateInstance(100, 100, g_type='barabasi_albert', edge=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4GxcE3_U6mA"
   },
   "source": [
    "For random decompositions, we randomly decompose the 100 nodes into 5 equally-sized subsets. Iteratively, we apply 10 decompositions in total and for each subproblem, we impose a time limit of 1 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "oeKd6JSkv8_c",
    "outputId": "c9a296f2-a022-4ed6-c6ef-16bacffac8b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective:  -155.62469740679254\n",
      "objective:  -165.36952461874057\n",
      "objective:  -171.8718668480746\n",
      "objective:  -173.53634467141407\n",
      "objective:  -173.9082557153236\n",
      "objective:  -173.9082557153237\n",
      "objective:  -176.08994042301055\n",
      "objective:  -176.0899404230104\n",
      "objective:  -176.08994042301043\n",
      "objective:  -176.0899404230105\n",
      "solver time:  1.4998536109924316\n",
      "total time:  3.400538682937622\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "t, _, _, _, _ = LNS(P, 5, 10, time_limit=1, verbose=True)\n",
    "end_time = time.time()\n",
    "total_t = end_time - start_time\n",
    "print('solver time: ', t)\n",
    "print('total time: ', total_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdqaEc61VZL3"
   },
   "source": [
    "To compare the solver's performance without decomposition, we give the solver 10 times the amount of wall-clock time to solve the problem and compare the final objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "OFMlO3X9wTWl",
    "outputId": "143b3dec-9cb9-4639-8f46-5d05bdd95d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN\n",
      "OptimizationStatus.FEASIBLE\n",
      "1.5758919715881348\n",
      "-162.95738511495105\n"
     ]
    }
   ],
   "source": [
    "status = P.optimize(max_seconds=total_t * 10)\n",
    "print(status)\n",
    "print(P.objective_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_mrpiqMVx-q"
   },
   "source": [
    "As the results show, even though the solver uses substantially more time, it produces worse solution compared with the version with decomposition."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LNS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
